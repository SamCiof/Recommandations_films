from components.base_component import BaseComponent
import streamlit as st

class AutoencodeurComponent(BaseComponent):
    def __init__(self):
        super().__init__("Modèle Autoencodeur")
    
    def display(self):
        st.header(self.title)
        st.markdown(self.get_model_test(), unsafe_allow_html=True)
        st.markdown(self.get_loading_and_preparing_data(), unsafe_allow_html=True)
        st.markdown(self.get_dimensionality_reduction(), unsafe_allow_html=True)
        st.markdown(self.get_construction_and_training_model(), unsafe_allow_html=True)
        st.markdown(self.get_model_evaluation(), unsafe_allow_html=True)
        st.markdown(self.get_final_model_test(), unsafe_allow_html=True)
        st.markdown(self.get_conclusion(), unsafe_allow_html=True)
    
    def get_model_test(self):
        return (""" 
        ### Test du modèle
        Dans le cadre de notre projet sur les systèmes de recommandation, nous avons tenté d’élaborer un modèle hybride combinant les techniques de filtrage basées sur le contenu 
        et le filtrage collaboratif. 
        Ce modèle utilise le deep learning et notamment autoencodeur. Notre objectif était de créer un système capable de fournir des recommandations à la fois précises et personnalisées, 
        en exploitant la puissance du deep learning. 
        Nous avons cherché à évaluer la performance de ce modèle, en comparant ses résultats avec d'autres méthodes plus traditionnelles, afin de déterminer dans quelle mesure le deep 
        learning pouvait améliorer l'efficacité des recommandations.
        """)
            
    def get_loading_and_preparing_data(self):
        return ("""
        **Chargement et Préparation des données :**\n
        Une fois les données chargées, nous avons entrepris plusieurs étapes de préparation pour optimiser leur utilisation dans notre modèle de recommandation.
        Tout d'abord, nous avons effectué les mêmes étapes de prétraitement que celles utilisées pour le modèle Annoy, afin de garantir une cohérence entre les deux modèles pour les comparer 
        entre eux. Cela implique la suppression de la colonne imdbId, l’ajout de la colonne weightedAverageRating, la conversion de toutes les colonnes pertinentes en entiers, et la sélection 
        de la colonne title comme cible.
        """)
    
    def get_dimensionality_reduction(self):
        return ("""
        **Réduction de la Dimensionnalité avec TruncatedSVD :**\n
        Nous avons opté pour le même nombre de composantes pour la décomposition par TruncatedSVD que le modèle Annoy, c’est à dire 100 composantes.
        """)
    
    def get_construction_and_training_model(self):
        return ("""
        **Construction et entraînement de l’Autoencodeur :**\n
        Après avoir déterminé le nombre optimal de composantes pour TruncatedSVD, nous avons procédé à la construction du modèle basé sur des autoencodeurs. Cette architecture comprenait des 
        couches d'encodeurs et de décodeurs, permettant de réduire la dimensionnalité des données tout en capturant les principales caractéristiques pour la recommandation.
        Afin de prévenir tout surapprentissage et d'améliorer l'efficacité de l'entraînement, nous avons intégré la technique d'EarlyStopping. Cette méthode surveille la performance du modèle 
        sur les données de validation et interrompt l'entraînement si les performances cessent de s'améliorer après un certain nombre d'époques. Ainsi, cela permet de conserver le meilleur 
        modèle obtenu avant que le processus d'entraînement ne commence à stagner, garantissant une utilisation optimale des ressources de calcul et une performance maximale du modèle.
        """)
    
    def get_model_evaluation(self):
        return ("""
        **Evaluation du modèle :**\n
        Après l'entraînement et l'application du modèle basé sur les autoencodeurs, nous avons procédé à son évaluation pour vérifier sa performance sur les données de test, notamment sur le 
        MSE (Mean Squared Error) et le Tess Loss. Et les résultats obtenus nous ont révélé que le modèle est performant, avec une faible perte sur les données de test et un MSE relativement bas. 
        """)
    
    def get_final_model_test(self):
        return ("""
        **Tests du Modèle :**\n
        Après avoir construit et optimisé notre modèle, nous avons procédé à une série de tests pour évaluer la qualité des recommandations générées. Malheureusement, les résultats obtenus n'ont 
        pas répondu à nos attentes. Les recommandations étaient souvent incohérentes, et chaque nouvel entraînement du modèle produisait des suggestions de films de manière très aléatoire.
        En outre, en comparaison avec le modèle Annoy, ce modèle s'est révélé plus lourd à charger et moins performant en termes de vitesse d'exécution et de stabilité des résultats.
        """)
    
    def get_conclusion(self):
        return ("""
        ### Conclusion
        En conséquence, bien que le deep learning présente des avantages théoriques, nous avons décidé de privilégier l'utilisation du modèle Annoy, qui s'est avéré plus rapide, plus léger, et capable de fournir des recommandations plus cohérentes et pertinentes.
        """)